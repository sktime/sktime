{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting with sktime\n",
    "\n",
    "In forecasting, we're interested in using past data to make temporal forward predictions. sktime provides common statistical forecasting algorithms and tools for building composite machine learning models.\n",
    "\n",
    "\n",
    "<img src=\"img/forecasting.png\" width=750 />\n",
    "\n",
    "For more details, take a look at [our paper on forecasting with sktime](https://arxiv.org/abs/2005.08067) in which we discuss the forecasting API in more detail and use it to replicate and extend the M4 study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sktime.datasets import load_airline\n",
    "from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "from sktime.performance_metrics.forecasting import smape_loss\n",
    "from sktime.utils.plotting.forecasting import plot_ys\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "For this tutorial, we will use the famous Box-Jenkins airline data set, which shows the number of international airline\n",
    "passengers per month from 1949-1960.\n",
    "\n",
    "As well as using the original time series (which is a classic example of a *multiplicative* time series), we will create an *additive* time series by performing a log-transform on the original data, so we may compare forecasters against both types of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = load_airline()\n",
    "fig, ax = plot_ys(y)\n",
    "ax.set(xlabel=\"Time\", ylabel=\"Number of airline passengers\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying the forecasting task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will define a forecasting task.\n",
    "\n",
    "* We will try to predict the last 3 years of data, using the previous years as training data. Each point in the series represents a month, so we should hold out the last 36 points as test data, and use 36-step ahead forecasting horizon to evaluate forecasting performance.\n",
    "* We will use the sMAPE (symmetric mean absolute percentage error) to quantify the accuracy of our forecasts. A lower sMAPE means higher accuracy.\n",
    "\n",
    "We can split the data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_test = temporal_train_test_split(y, test_size=36)\n",
    "plot_ys(y_train, y_test, labels=[\"y_train\", \"y_test\"])\n",
    "print(y_train.shape[0], y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we want to generate forecasts, we need to specify the forecasting horizon and pass that to our forecasting algorithm. We can specify the forecasting horizon as a simple numpy array of the steps ahead, relative to the end of the training series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fh = np.arange(len(y_test)) + 1\n",
    "fh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here we're interested in predicting from the first to to the 36th step ahead.\n",
    "\n",
    "Of course you could you use other forecasting horizons. For example, to predict only the second and fifth step ahead, you could write:\n",
    "\n",
    "```python\n",
    "fh = np.array([2, 5])  # 2nd and 5th step ahead\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting\n",
    "\n",
    "Like in scikit-learn, in order to make forecasts, we need to first specify (or build) a model, then fit it to the training data, and finally call predict to generate forecasts for the given forecasting horizon.\n",
    "\n",
    "sktime comes with several forecasting algorithms (or forecasters) and tools for composite model building. All forecaster share a common interface. Forecasters are trained on a single series of data and make forecasts for the provided forecasting horizon.\n",
    "\n",
    "### Naïve baselines\n",
    "Let's start with two naïve forecasting strategies which can serve as references for comparison of more sophisticated approaches.\n",
    "\n",
    "1. We always predict the last value observed (in the training series),\n",
    "2. We predict the last value observed in the same season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can do that with a few lines of code\n",
    "y_pred = np.repeat(y_train.iloc[-1], len(fh))\n",
    "y_pred = pd.Series(y_pred, index=y_train.index[-1] + fh)\n",
    "\n",
    "plot_ys(y_train, y_test, y_pred, labels=[\"y_train\", \"y_test\", \"y_pred\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using sktime \n",
    "\n",
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "forecaster = NaiveForecaster(strategy=\"last\")\n",
    "forecaster.fit(y_train)\n",
    "y_last = forecaster.predict(fh)\n",
    "plot_ys(y_train, y_test, y_pred, labels=[\"y_train\", \"y_test\", \"y_pred\"]);\n",
    "smape_loss(y_last, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecaster = NaiveForecaster(strategy=\"last\", sp=12)\n",
    "forecaster.fit(y_train)\n",
    "y_pred = forecaster.predict(fh)\n",
    "plot_ys(y_train, y_test, y_pred, labels=[\"y_train\", \"y_test\", \"y_pred\"])\n",
    "smape_loss(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But can I not just use scikit-learn?\n",
    "\n",
    "In principle, yes, but many pitfalls ...\n",
    "\n",
    "### Pitfall 1: model validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y_train, y_test = train_test_split(y)\n",
    "plot_ys(y_train.sort_index(), y_test.sort_index(), labels=[\"y_train\", \"y_test\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leads to leakage: \n",
    "\n",
    "> The data you are using to train a machine learning algorithm happens to have the information you are trying to predict.\n",
    "\n",
    "But `train_test_split(y, shuffle=False)` works, which is what `temporal_train_test_split(y)` does in sktime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_test = temporal_train_test_split(y)\n",
    "plot_ys(y_train, y_test, labels=[\"y_train\", \"y_test\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pitfall 2: how to apply regression algorithms?\n",
    "\n",
    "In order to use scikit-learn, we have to first transform the data into the required tabular format, then fit a regressor and finally generate forecasts. \n",
    "\n",
    "#### Reduction: from forecasting to regression\n",
    "Forecasting is often solved via regression. This approach is sometimes called reduction, because we reduce the forecasting task to the simpler but related task of tabular regression. This allows to apply any regression algorithm to the forecasting problem.\n",
    "\n",
    "Reduction to regression works as follows: We first need to transform the data into the required tabular format. We can do this by cutting the training series into windows of a fixed length and stacking them on top of each other. Our target variable consists of the subsequent observation for each window.\n",
    "\n",
    "We could write some code to do that, as for example in the [M4 competition](https://github.com/Mcompetitions/M4-methods):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slightly modified code from the M4 competition\n",
    "def split_into_train_test(data, in_num, fh):\n",
    "    \"\"\"\n",
    "    Splits the series into train and test sets. Each step takes multiple points as inputs\n",
    "    :param data: an individual TS\n",
    "    :param fh: number of out of sample points\n",
    "    :param in_num: number of input points for the forecast\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    train, test = data[:-fh], data[-(fh + in_num):]\n",
    "    x_train, y_train = train[:-1], np.roll(train, -in_num)[:-in_num]\n",
    "    x_test, y_test = test[:-1], np.roll(test, -in_num)[:-in_num]\n",
    "#     x_test, y_test = train[-in_num:], np.roll(test, -in_num)[:-in_num]\n",
    "\n",
    "    # reshape input to be [samples, time steps, features] (N-NF samples, 1 time step, 1 feature)\n",
    "    x_train = np.reshape(x_train, (-1, 1))\n",
    "    x_test = np.reshape(x_test, (-1, 1))\n",
    "    temp_test = np.roll(x_test, -1)\n",
    "    temp_train = np.roll(x_train, -1)\n",
    "    for x in range(1, in_num):\n",
    "        x_train = np.concatenate((x_train[:-1], temp_train[:-1]), 1)\n",
    "        x_test = np.concatenate((x_test[:-1], temp_test[:-1]), 1)\n",
    "        temp_test = np.roll(temp_test, -1)[:-1]\n",
    "        temp_train = np.roll(temp_train, -1)[:-1]\n",
    "\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we split the time index, rather than the actual values, to show how we split the windows\n",
    "feature_window, target_window, _, _ = split_into_train_test(y.index.values, 10, len(fh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_window[:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_window[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can split the actual values of the time series\n",
    "x_train, y_train, x_test, y_test = split_into_train_test(y.values, 10, len(fh))\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor()\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are potential pitfalls here?\n",
    "\n",
    "> This requires a lot of hand-written code which is often error-prone, not modular and not tuneable. \n",
    "\n",
    "> Note also that these steps involve a number of implicit hyper-parameters:\n",
    "* the way you slice the time series into windows (e.g. the window length)\n",
    "* the way you generate forecasts (recursive strategy, direct strategy, other hybrid strategies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pitfall 3: how to generate forecasts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_test.shape, y_test.shape)\n",
    "\n",
    "# add back time index to y_test \n",
    "y_test = pd.Series(y_test, index=y.index[-len(fh):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "smape_loss(pd.Series(y_pred, index=y_test.index), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what's the problem here?\n",
    "\n",
    "> We actually don't make a multi-step-ahead forecast up to the 36th step ahead. Instead, we make 36 single-step-ahead forecasts always using the most recent data. But that's a solution to a different learning task! \n",
    "\n",
    "To fix this problem, we could write some code to do this recursively as in the M4 competition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slightly modified code from the M4 study\n",
    "predictions = []\n",
    "last_window = x_train[-1, :].reshape(1, -1)  # make it into 2d array\n",
    "\n",
    "last_prediction = model.predict(last_window)[0]  # take value from array\n",
    "\n",
    "for i in range(len(fh)):\n",
    "    # append prediction\n",
    "    predictions.append(last_prediction)\n",
    "    \n",
    "    # update last window using previously predicted value\n",
    "    last_window[0] = np.roll(last_window[0], -1)\n",
    "    last_window[0, (len(last_window[0]) - 1)] = last_prediction\n",
    "    \n",
    "    # predict next step ahead\n",
    "    last_prediction = model.predict(last_window)[0]\n",
    "\n",
    "y_pred_rec = pd.Series(predictions, index=y_test.index)\n",
    "smape_loss(y_pred_rec, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting with sktime\n",
    "\n",
    "### Reduction: from forecasting to regression\n",
    "\n",
    "sktime provides a meta-estimator for this approach, which is: \n",
    "* **modular** and **compatible with scikit-learn**, so that we can easily apply any scikit-learn regressor to solve our forecasting problem,\n",
    "* **tuneable**, allowing us to tune hyper-parameters like the window length or strategy to generate forecasts\n",
    "* **adaptive**, in the sense that it adapts the scikit-learn's estimator interface to that of a forecaster, making sure that we can tune and properly evaluate our model\n",
    "\n",
    "<img src=\"img/forecasting-to-regression-reduction.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = load_airline()\n",
    "y_train, y_test = temporal_train_test_split(y, test_size=36)\n",
    "print(y_train.shape[0], y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.forecasting.compose import ReducedRegressionForecaster\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "regressor = KNeighborsRegressor(n_neighbors=1)\n",
    "forecaster = ReducedRegressionForecaster(regressor=regressor, window_length=12, strategy=\"recursive\")\n",
    "forecaster.fit(y_train)\n",
    "y_pred = forecaster.predict(fh)\n",
    "plot_ys(y_train, y_test, y_pred, labels=[\"y_train\", \"y_test\", \"y_pred\"]);\n",
    "smape_loss(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand the prior data transformation, we can look at how we can split the training series into windows. Internally, sktime uses a temporal time series splitter, similar to the cross-validation splitter in scikit-learn. Here we show how this works for the first 20 observations of the training series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sktime.forecasting.model_selection import SlidingWindowSplitter\n",
    "cv = SlidingWindowSplitter(window_length=10, start_with_window=True)\n",
    "for input_window, output_window in cv.split(y_train.iloc[:20]):\n",
    "    print(input_window, output_window)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical forecasters\n",
    "\n",
    "sktime has a number of statistical forecasting algorithms, based on implementations in statsmodels. For example, to use exponential smoothing with an additive trend component and multiplicative seasonality, we can write the following.\n",
    "\n",
    "Note that since this is monthly data, the seasonal periodicity (sp), or the number of periods per year, is 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.forecasting.exp_smoothing import ExponentialSmoothing\n",
    "forecaster = ExponentialSmoothing(trend=\"add\", seasonal=\"multiplicative\", sp=12)\n",
    "forecaster.fit(y_train)\n",
    "y_pred = forecaster.predict(fh)\n",
    "plot_ys(y_train, y_test, y_pred, labels=[\"y_train\", \"y_test\", \"y_pred\"]);\n",
    "smape_loss(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exponential smoothing of state space model can also be automated similar to the [ets](https://www.rdocumentation.org/packages/forecast/versions/8.13/topics/ets) function in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.forecasting.ets import AutoETS\n",
    "forecaster = AutoETS(autofitting=True,sp=12,n_jobs=-1,allow_multiplicative_trend=True)\n",
    "forecaster.fit(y_train.astype('float64'))\n",
    "y_pred = forecaster.predict(fh)\n",
    "plot_ys(y_train, y_test, y_pred, labels=[\"y_train\", \"y_test\", \"y_pred\"]);\n",
    "smape_loss(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common model is the ARIMA model. In sktime, we interface [pmdarima](https://github.com/alkaline-ml/pmdarima), a package for automatically selecting the best ARIMA model. This since searches over a number of possible model parametrisations, it may take a bit longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.forecasting.arima import AutoARIMA\n",
    "forecaster = AutoARIMA(sp=12, suppress_warnings=True)\n",
    "forecaster.fit(y_train)\n",
    "y_pred = forecaster.predict(fh)\n",
    "plot_ys(y_train, y_test, y_pred, labels=[\"y_train\", \"y_test\", \"y_pred\"]);\n",
    "smape_loss(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compositite model building\n",
    "\n",
    "sktime provides a modular API for composite model building for forecasting.\n",
    "\n",
    "### Ensembling\n",
    "Like scikit-learn, sktime provides a meta-forecaster to ensemble multiple forecasting algorithms. For example, we can combine different variants of exponential smoothing as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sktime.forecasting.compose import EnsembleForecaster\n",
    "forecaster = EnsembleForecaster([\n",
    "    (\"ses\", ExponentialSmoothing(seasonal=\"multiplicative\", sp=12)),\n",
    "    (\"holt\", ExponentialSmoothing(trend=\"add\", damped=False, seasonal=\"multiplicative\", sp=12)),\n",
    "    (\"damped\", ExponentialSmoothing(trend=\"add\", damped=True, seasonal=\"multiplicative\", sp=12))\n",
    "])\n",
    "forecaster.fit(y_train)\n",
    "y_pred = forecaster.predict(fh)\n",
    "plot_ys(y_train, y_test, y_pred, labels=[\"y_train\", \"y_test\", \"y_pred\"]);\n",
    "smape_loss(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning\n",
    "In the `ReducedRegressionForecaster`, both the `window_length` and `strategy` arguments are hyper-parameters which we may want to optimise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.forecasting.model_selection import ForecastingGridSearchCV\n",
    "\n",
    "forecaster = ReducedRegressionForecaster(regressor=regressor, window_length=15, strategy=\"recursive\")\n",
    "param_grid = {\"window_length\": [5, 10, 15]}\n",
    "\n",
    "# we fit the forecaster on the initial window, and then use temporal cross-validation to find the optimal parameter\n",
    "cv = SlidingWindowSplitter(initial_window=int(len(y_train) * 0.5))\n",
    "gscv = ForecastingGridSearchCV(forecaster, cv=cv, param_grid=param_grid)\n",
    "gscv.fit(y_train)\n",
    "y_pred = gscv.predict(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ys(y_train, y_test, y_pred, labels=[\"y_train\", \"y_test\", \"y_pred\"]);\n",
    "smape_loss(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using scikit-learn's `GridSearchCV`, we can tune regressors imported from scikit-learn, in addition to tuning `window_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sktime.forecasting.compose import RecursiveRegressionForecaster\n",
    "\n",
    "# tuning the 'n_estimator' hyperparameter of RandomForestRegressor from scikit-learn\n",
    "regressor_param_grid = {\"n_estimators\": [100, 200, 300]}\n",
    "forecaster_param_grid = {\"window_length\": [5,10,15,20,25]}\n",
    "\n",
    "# create a tunnable regressor with GridSearchCV\n",
    "regressor = GridSearchCV(RandomForestRegressor(), param_grid=regressor_param_grid)\n",
    "forecaster = RecursiveRegressionForecaster(regressor, window_length=15)\n",
    "\n",
    "cv = SlidingWindowSplitter(initial_window=int(len(y_train) * 0.5))\n",
    "gscv = ForecastingGridSearchCV(forecaster, cv=cv, param_grid=forecaster_param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv.fit(y_train)\n",
    "y_pred = gscv.predict(fh)\n",
    "plot_ys(y_train, y_test, y_pred, labels=[\"y_train\", \"y_test\", \"y_pred\"]);\n",
    "smape_loss(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gscv.best_params_, gscv.best_forecaster_.regressor_.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access performance on a particular metric during tuning, we can use the `scoring` argument of `ForecastingGridSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.performance_metrics.forecasting import sMAPE\n",
    "\n",
    "gscv = ForecastingGridSearchCV(forecaster, cv=cv, param_grid=forecaster_param_grid, scoring=sMAPE())\n",
    "gscv.fit(y_train)\n",
    "print(gscv.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detrending\n",
    "Note that so far the reduction approach above does not take any seasonal or trend into account, but we can easily specify a pipeline which first detrends the data.\n",
    "\n",
    "sktime provides a generic detrender, a transformer which uses any forecaster and returns the in-sample residuals of the forecaster's predicted values. For example, to remove the linear trend of a time series, we can write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.forecasting.trend import PolynomialTrendForecaster\n",
    "from sktime.transformers.single_series.detrend import Detrender\n",
    "\n",
    "# liner detrending\n",
    "forecaster = PolynomialTrendForecaster(degree=1)\n",
    "transformer = Detrender(forecaster=forecaster)\n",
    "yt = transformer.fit_transform(y_train)\n",
    "\n",
    "# internally, the Detrender uses the in-sample predictions of the PolynomialTrendForecaster\n",
    "forecaster = PolynomialTrendForecaster(degree=1)\n",
    "fh_ins = -np.arange(len(y_train)) # in-sample forecasting horizon\n",
    "y_pred = forecaster.fit(y_train).predict(fh=fh_ins)\n",
    "\n",
    "plot_ys(y_train, y_pred, yt, labels=[\"y_train\", \"Fitted linear trend\", \"Residuals\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelining\n",
    "\n",
    "Let's use the detrender in a pipeline together with de-seasonalisation. Note that in forecasting, when we apply data transformations before fitting, we need to apply the inverse transformation to the predicted values. For this purpose, we provide the following pipeline class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.forecasting.compose import TransformedTargetForecaster\n",
    "from sktime.transformers.single_series.detrend import Deseasonalizer\n",
    "\n",
    "forecaster = TransformedTargetForecaster([\n",
    "    (\"deseasonalise\", Deseasonalizer(model=\"multiplicative\", sp=12)),\n",
    "    (\"detrend\", Detrender(forecaster=PolynomialTrendForecaster(degree=1))),\n",
    "    (\"forecast\", ReducedRegressionForecaster(regressor=regressor, window_length=15, strategy=\"recursive\"))\n",
    "])\n",
    "forecaster.fit(y_train)\n",
    "y_pred = forecaster.predict(fh)\n",
    "plot_ys(y_train, y_test, y_pred, labels=[\"y_train\", \"y_test\", \"y_pred\"]);\n",
    "smape_loss(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we could try again to optimise the hyper-parameters of components of the pipeline.\n",
    "\n",
    "Below we discuss two other aspects of forecasting: online learning, where we want to dynamically update forecasts as new data comes in, and prediction intervals, which allow us to quantify the uncertainty of our forecasts.\n",
    "\n",
    "## Dynamic forecasts\n",
    "\n",
    "For model evaluation, we sometimes want to evaluate multiple forecasts, using temporal cross-validation with a sliding window over the test data. For this purpose, all forecasters in sktime have a `update_predict` method. Here we make repeated single-step ahead forecasts over the test set.\n",
    "\n",
    "Note that the forecasting task is changed: while we still make 36 predictions, we do not predict 36 steps ahead, but instead make 36 single-step-ahead predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecaster = NaiveForecaster(strategy=\"last\")\n",
    "forecaster.fit(y_train)\n",
    "cv = SlidingWindowSplitter(fh=1)\n",
    "y_pred = forecaster.update_predict(y_test, cv)\n",
    "smape_loss(y_test, y_pred)\n",
    "plot_ys(y_train, y_test, y_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a single update, you can use the `update` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction intervals\n",
    "So far, we've only looked at point forecasts. In many cases, we're also interested in prediction intervals. sktime's interface support prediction intervals, but we haven't implemented them for all algorithms yet.\n",
    "\n",
    "Here, we use the Theta forecasting algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.forecasting.theta import ThetaForecaster\n",
    "forecaster = ThetaForecaster(sp=12)\n",
    "forecaster.fit(y_train)\n",
    "alpha = 0.05  # 95% prediction intervals\n",
    "y_pred, pred_ints = forecaster.predict(fh, return_pred_int=True, alpha=alpha)\n",
    "smape_loss(y_test, y_pred)\n",
    "\n",
    "fig, ax = plot_ys(y_train, y_test, y_pred, labels=[\"y_train\", \"y_test\", \"y_pred\"])\n",
    "ax.fill_between(y_pred.index, pred_ints[\"lower\"], pred_ints[\"upper\"], alpha=0.2, color=\"green\", label=f\"{1 - alpha}% prediction intervals\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "As we have seen, in order to make forecasts, we need to first specify (or build) a model, then fit it to the training data, and finally call predict to generate forecasts for the given forecasting horizon. \n",
    "\n",
    "* sktime comes with several forecasting algorithms (or forecasters) and tools for composite model building. All forecaster share a common interface. Forecasters are trained on a single series of data and make forecasts for the provided forecasting horizon.\n",
    "\n",
    "* sktime has a number of statistical forecasting algorithms, based on implementations in statsmodels. For example, to use exponential smoothing with an additive trend component and multiplicative seasonality, we can write the following.\n",
    "\n",
    "\n",
    "## Useful resources\n",
    "* For more details, take a look at [our paper on forecasting with sktime](https://arxiv.org/abs/2005.08067) in which we discuss the forecasting API in more detail and use it to replicate and extend the M4 study.\n",
    "* For a good introduction to forecasting, see [Hyndman, Rob J., and George Athanasopoulos. Forecasting: principles and practice. OTexts, 2018](https://otexts.com/fpp2/).\n",
    "* For comparative benchmarking studies/forecasting competitions, see the [M4 competition](https://www.sciencedirect.com/science/article/pii/S0169207019301128) and the currently running [M5 competition](https://www.kaggle.com/c/m5-forecasting-accuracy/overview)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
