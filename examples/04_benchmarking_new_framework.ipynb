{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking with sktime\n",
    "\n",
    "The benchmarking modules allows you to easily orchestrate benchmarking experiments in which you want to compare the performance of one or more algorithms over one or more data sets. It also provides a number of statistical tests to check if observed performance differences are statistically significant.\n",
    "\n",
    "\n",
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-09T13:57:12.800774Z",
     "iopub.status.busy": "2021-04-09T13:57:12.800145Z",
     "iopub.status.idle": "2021-04-09T13:57:13.736550Z",
     "shell.execute_reply": "2021-04-09T13:57:13.736911Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benediktheidrich/code/sktime/venv/lib/python3.10/site-packages/gluonts/json.py:102: UserWarning: Using `json`-module for json-handling. Consider installing one of `orjson`, `ujson` to speed up serialization and deserialization.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# import required functions and classes\n",
    "import warnings\n",
    "\n",
    "from sktime.benchmarking.forecasting_new import (\n",
    "    ForecastingBenchmark as ForecastingBenchmarkNew,\n",
    ")\n",
    "from sktime.datasets import load_airline\n",
    "from sktime.forecasting.moirai_forecaster import MOIRAIForecaster\n",
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "from sktime.forecasting.reconcile import ReconcilerForecaster\n",
    "from sktime.forecasting.ttm import TinyTimeMixerForecaster\n",
    "from sktime.performance_metrics.forecasting import (\n",
    "    MeanAbsoluteError,\n",
    ")\n",
    "from sktime.split import SlidingWindowSplitter\n",
    "\n",
    "# hide warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Time Series Forecasting Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-09T13:57:13.745627Z",
     "iopub.status.busy": "2021-04-09T13:57:13.745080Z",
     "iopub.status.idle": "2021-04-09T13:57:13.747037Z",
     "shell.execute_reply": "2021-04-09T13:57:13.747533Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TinyTimeMixerForecaster\n",
      "[ScoreResult(name='MeanAbsoluteError', score=array([60.63170647])), ScoreResult(name='fit_time', score=0.8591277083323803), ScoreResult(name='pred_time', score=0.006748305330499231)]\n",
      "---\n",
      "MOIRAIForecaster\n",
      "[ScoreResult(name='MeanAbsoluteError', score=array([52.09134558])), ScoreResult(name='fit_time', score=0.23429156933707418), ScoreResult(name='pred_time', score=0.03190738899623587)]\n",
      "---\n",
      "NaiveForecaster\n",
      "[ScoreResult(name='MeanAbsoluteError', score=array([76.96969697])), ScoreResult(name='fit_time', score=0.0010273609999179218), ScoreResult(name='pred_time', score=0.004412986333287942)]\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "benchmark = ForecastingBenchmarkNew()\n",
    "\n",
    "\n",
    "## Add estimators\n",
    "benchmark.add_estimator(TinyTimeMixerForecaster())\n",
    "benchmark.add_estimator(MOIRAIForecaster(\"sktime/moirai-1.0-R-small\"))\n",
    "\n",
    "scorers = [MeanAbsoluteError(multioutput=\"raw_values\")]\n",
    "\n",
    "benchmark.add_task(\n",
    "    load_airline,\n",
    "    SlidingWindowSplitter(range(1, 12), 108, 12),\n",
    "    scorers,\n",
    ")\n",
    "\n",
    "benchmark_result = benchmark.run(\n",
    "    \"./benchmarking_results.json\",\n",
    ")\n",
    "for result in benchmark_result.results:\n",
    "    print(result.model_id)\n",
    "    print(result.means)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-09T13:57:13.750377Z",
     "iopub.status.busy": "2021-04-09T13:57:13.749904Z",
     "iopub.status.idle": "2021-04-09T13:57:13.751552Z",
     "shell.execute_reply": "2021-04-09T13:57:13.752038Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TinyTimeMixerForecaster\n",
      "[ScoreResult(name='MeanAbsoluteError', score=array([60.63170647])), ScoreResult(name='fit_time', score=0.8591277083323803), ScoreResult(name='pred_time', score=0.006748305330499231)]\n",
      "---\n",
      "MOIRAIForecaster\n",
      "[ScoreResult(name='MeanAbsoluteError', score=array([52.09134558])), ScoreResult(name='fit_time', score=0.23429156933707418), ScoreResult(name='pred_time', score=0.03190738899623587)]\n",
      "---\n",
      "NaiveForecaster\n",
      "[ScoreResult(name='MeanAbsoluteError', score=array([76.96969697])), ScoreResult(name='fit_time', score=0.0010273609999179218), ScoreResult(name='pred_time', score=0.004412986333287942)]\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "benchmark.add_estimator(NaiveForecaster(strategy=\"last\"))\n",
    "\n",
    "benchmark_result = benchmark.run(\n",
    "    \"./benchmarking_results.json\",\n",
    ")\n",
    "\n",
    "for result in benchmark_result.results:\n",
    "    print(result.model_id)\n",
    "    print(result.means)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierachical Forecastin Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-09T13:57:13.755084Z",
     "iopub.status.busy": "2021-04-09T13:57:13.754630Z",
     "iopub.status.idle": "2021-04-09T13:57:13.756086Z",
     "shell.execute_reply": "2021-04-09T13:57:13.756565Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReconcilerForecaster\n",
      "[ScoreResult(name='MeanAbsoluteError', score=array([[190.42989889],\n",
      "       [ 10.        ],\n",
      "       [136.42135635],\n",
      "       [ 44.00854254]])), ScoreResult(name='fit_time', score=0.03225533350268961), ScoreResult(name='pred_time', score=0.01662606250101817)]\n",
      "---\n",
      "Reconciler_2\n",
      "[ScoreResult(name='MeanAbsoluteError', score=array([[145.99613686],\n",
      "       [  7.66666667],\n",
      "       [104.58962157],\n",
      "       [ 33.73984863]])), ScoreResult(name='fit_time', score=0.08565895849824301), ScoreResult(name='pred_time', score=0.008330624998052372)]\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "benchmark = ForecastingBenchmarkNew()\n",
    "\n",
    "forecaster = NaiveForecaster(strategy=\"last\")\n",
    "reconciler_1 = ReconcilerForecaster(forecaster, method=\"mint_shrink\")\n",
    "\n",
    "forecaster = NaiveForecaster(strategy=\"drift\")\n",
    "reconciler_2 = ReconcilerForecaster(forecaster, method=\"mint_shrink\")\n",
    "\n",
    "\n",
    "benchmark.add_estimator(reconciler_1)\n",
    "benchmark.add_estimator(reconciler_2, estimator_id=\"Reconciler_2\")\n",
    "\n",
    "\n",
    "# TODO Data generation needs to be prettier\n",
    "from sktime.transformations.hierarchical.aggregate import Aggregator\n",
    "from sktime.utils._testing.hierarchical import _bottom_hier_datagen\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    agg = Aggregator()\n",
    "\n",
    "    y = _bottom_hier_datagen(\n",
    "        no_bottom_nodes=3,\n",
    "        no_levels=1,\n",
    "        random_seed=123,\n",
    "        length=9,\n",
    "    )\n",
    "\n",
    "    y = agg.fit_transform(y)\n",
    "    return y\n",
    "\n",
    "\n",
    "scorers = [MeanAbsoluteError(multilevel=\"raw_values\")]\n",
    "\n",
    "splitter = SlidingWindowSplitter(fh=[1, 2, 3], window_length=4, step_length=2)\n",
    "\n",
    "benchmark.add_task(\n",
    "    get_data,\n",
    "    splitter,\n",
    "    scorers,\n",
    ")\n",
    "\n",
    "benchmark_result = benchmark.run(\n",
    "    \"./hierachical_benchmark.json\",\n",
    ")\n",
    "\n",
    "for result in benchmark_result.results:\n",
    "    print(result.model_id)\n",
    "    print(result.means)\n",
    "    print(\"---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
