# -*- coding: utf-8 -*-
"""
HMM Annotation Estimator.

Implements a basic Hidden Markov Model (HMM) as an annotation estimator.
To read more about the algorithm, check out the `HMM wikipedia page
<https://en.wikipedia.org/wiki/Hidden_Markov_model>`_.
"""
from typing import Tuple

import numpy as np

from sktime.annotation.base._base import BaseSeriesAnnotator

__author__ = ["miraep8"]
__all__ = ["HMM"]


class HMM(BaseSeriesAnnotator):
    """Implements a simple HMM fitted with Viterbi algorithm.

    The HMM annotation estimator uses the
    the Viterbi algorithm to fit a sequence of 'hidden state' class
    annotations (represented by an array of integers the same size
    as the observation) to a sequence of observations.

    This is done by finding the most likely path given the emission
    probabilities - (ie the probability that a particular observation
    would be generated by a given hidden state), the transition prob
    (ie the probability of transitioning from one state to another or
    staying in the same state) and the initial probabilities - ie the
    belief of the probability distribution of hidden states at the
    start of the observation sequence).

    _fit is currently empty as the parameters of the probability
    distribution are required to be passed to the algorithm.

    _predict - first the transition_probability and transition_id matrices are
    calculated - these are both nxm matrices, where n is the number of
    hidden states and m is the number of observations. The transition
    probability matrices record the probability of the most likely
    sequence which has observation `m` being assigned to hidden state n.
    The transition_id's record the step before hidden state n that
    proceeds it in the most likely path.  This logic is mostly carried
    out by helper function _calculate_trans_mats.
    Next, these matrices are used to calculate the most likely
    path (by backtracing from the final mostly likely state and the
    id's that proceeded it.)  This logic is done via a helper func
    hmm_viterbi_label.

    Parameters
    ----------
    emission_funcs : a list of length n.  Should be either a list of
        functions (callables, eg [func1, func2.....] - which take a
        value and return a probability and have a signature of the form
        func(X) -> float, or a list of function (callable) and keyword argument
        dictionary tuples [(func, kwarg), (func, kwarg)], where each function
        returns a probability when passed a single observation and
        whatever arguments are supplied in the respective keyword arguent
        dictionary (ie func(X, **kwarg) -> float) All functions should be properly
        normalized PDFs over the same space as the observed data.
    transition_prob_mat: np.ndarry a nxn array of probabilities, where 'n'
        represents the number of hidden states in the HMM.  Each
        row should sumn to 1 in order to be properly normalized
        (ie the j'th column in the i'th row represents the
        probability of transitioning from state i to state j.)
    initial_probs: np.ndarray (optional) a array of length n of probabilities over
        the starting states. The length should match the length of both
        the emission funcs list and the transition_prob_mat.
        The initial probs should be reflective of prior beliefs.  If none
        is passed will each hidden state will be assigned an equal inital
        prob.

    Examples
    --------
    >>> from sktime.annotation.hmm import HMM
    >>> from scipy.stats import norm
    >>> from numpy import asarray
    >>> # define the emission probs for our HMM model:
    >>> centers = [3.5,-5]
    >>> sd = [.25 for i in centers]
    >>> emi_funcs = [(norm.pdf, {'loc': mean,
    ...  'scale': sd[ind]}) for ind, mean in enumerate(centers)]
    >>> hmm_est = HMM(emi_funcs, asarray([[0.25,0.75], [0.666, 0.333]]))
    >>> # generate synthetic data (or of course use your own!)
    >>> obs = asarray([3.7,3.2,3.4,3.6,-5.1,-5.2,-4.9])
    >>> hmm_est = hmm_est.fit(obs)
    >>> labels = hmm_est.predict(obs)
    """

    # plan to update to make multivariate.
    _tags = {"univariate-only": True, "fit_is_empty": True}

    def __init__(
        self,
        emission_funcs: list,
        transition_prob_mat: np.ndarray,
        initial_probs: np.ndarray = None,
    ):
        self.initial_probs = initial_probs
        self.emission_funcs = emission_funcs
        self.transition_prob_mat = transition_prob_mat
        super(HMM, self).__init__(fmt="dense", labels="int_label")
        self._validate_init()

    def _validate_init(self):
        """Verify that the parameters passed to init are well behaved."""
        tran_mat_len = self.transition_prob_mat.shape[0]
        # transition_prob_mat should be square:
        if (
            not self.transition_prob_mat.ndim == 2
            or not tran_mat_len == self.transition_prob_mat.shape[1]
        ):
            raise ValueError(
                "Transtion Probability must be 2D square, but got an"
                f"object of size {self.transition_prob_mat.shape}"
            )
        # number of states should be consistent!
        if self.initial_probs is not None:
            init_prob_len = len(self.initial_probs)
        else:
            # if init-prob_lens is None, it will be generated with this len:
            init_prob_len = len(self.emission_funcs)
        if not tran_mat_len == len(self.emission_funcs) == init_prob_len:
            raise ValueError(
                "Number of hidden states is inconsistent!  emission_funcs "
                f" was of length {len(self.emission_funcs)} transition_prob_mat was "
                f" of length {tran_mat_len} and the length of the passed "
                f" (or generated) list of initial probabilities was "
                f" {init_prob_len}. All of these lengths should be the same"
                f" as they all correspond to the same underlying list of hidden"
                f" states."
            )
        # sum of all rows in transition_prob_mat should be 1.
        if not np.isclose(
            np.ones(tran_mat_len),
            np.sum(self.transition_prob_mat, axis=1),
            rtol=5e-2,
        ).any():
            raise ValueError("The sum of all rows in the transition matrix must be 1.")
        # sum of all initial_probs should be 1 if it is provided.
        if self.initial_probs is not None and not sum(self.initial_probs) == 1:
            raise ValueError("Sum of initial probs should be 1.")

    @classmethod
    def _calculate_trans_mats(
        cls,
        initial_probs: np.ndarray,
        emi_probs: np.ndarray,
        transition_prob_mat: np.ndarray,
        num_obs: int,
        num_states: int,
    ) -> Tuple[np.array, np.array]:
        """Calculate the transition mats used in the Viterbi algorithm.

        Parameters
        ----------
        initial_probs : (np.ndarray of float) - A nx1 dimensional array where n
            represents the number of hidden states in the model. It
            contains the probability that hidden state for the state
            before the first observation was state n.  Should sum to 1.
        emi_probs : (np.ndarray of float)- A nxm dimensional array, where n is the
            number of hidden states and m is the number of observations.
            For a given observation, it will provide the probability for
            each of the hidden states that they could have given rise to that
            observation. Each entry should be beteen 0 and 1
        transition_prob_mat : (np.ndarray) - A nxn dimensional array where n is
            the number of hidden states in the model. The jth col in the ith row
            represents the probability of transitioning to state j from state i.
            Thus each row should sum to 1.
        num_obs : (int) the number of observations (m)
        num_states : (int) the number of hidden states (n)

        Returns
        -------
        trans_prob : (np.ndarray) an nxm dimensional array which represents the
            maximum probability of the hidden state of observation m is state n.
        trans_id : (np.ndarray) a nxm dimensional array which for each observation
            "i" and state "j" the i,j entry records the state_id of the most
            likely state that could have led to the hidden state being "i" for
            observation "j".
        """
        # trans_prob represents the maximum probability of being in that
        # state at that stage
        trans_prob = np.zeros((num_states, num_obs))
        trans_prob[:, 0] = np.log(initial_probs)

        # trans_id is the index of the state that would have been the most
        # likely preceeding state.
        trans_id = np.zeros((num_states, num_obs), dtype=np.int32)

        # use Vertibi Algorithm to fill in trans_prob and trans_id:
        for i in range(1, num_obs):
            # use log probabilities to try to keep nums reasonable -Inf
            # means 0 probability
            paths = np.zeros((num_states, num_states))
            for j in range(num_states):
                paths[j, :] += trans_prob[:, i - 1]  # adds prev trans_prob column-wise
                paths[:, j] += np.log(emi_probs[:, i])  # adds log(probs_sub) row-wise
            paths += np.log(
                transition_prob_mat
            )  # adds log(transition_prob_mat) element-wise
            trans_id[:, i] = np.argmax(paths, axis=0)
            trans_prob[:, i] = np.max(paths, axis=0)

        if np.any(np.isinf(trans_prob[:, -1])):
            raise ValueError("Change parameters, the distribution doesn't work")

        return trans_prob, trans_id

    @classmethod
    def _make_emission_probs(
        cls, emission_funcs: list, observations: np.ndarray
    ) -> np.ndarray:
        """Calculate the prob each obs comes from each hidden state."""
        # assign emission probabilities from each state to each position:

        emi_probs = np.zeros(shape=(len(emission_funcs), len(observations)))
        for state_id, emission_func in enumerate(emission_funcs):
            if isinstance(emission_func, tuple):
                emission_func = emission_func[0]
                kwargs = emission_func[1]
                emi_probs[state_id, :] = np.array(
                    [emission_func(x, **kwargs) for x in observations]
                )
            else:
                emi_probs[state_id, :] = np.array(
                    [emission_func(x) for x in observations]
                )
        return emi_probs

    def _hmm_viterbi_label(self) -> np.array:
        """Assign hidden state ids to all observations based on most likely path.

        Parameters
        ----------
        self: (HMM) an HMM instance which already has already had trans_id and trans_mat
            calculated.

        Returns
        -------
        hmm_fit: (np.ndarray) an mx1 array where m is the length of the X (obs).
            each entry in the array is an int representing a hidden id state
            that has been assigned to that observation.
        """
        hmm_fit = np.zeros(self.num_obs)
        # Now we trace backwards and find the most likely path:
        max_inds = np.zeros(self.num_obs, dtype=np.int32)
        max_inds[-1] = np.argmax(self.trans_prob[:, -1])
        hmm_fit[-1] = self.states[max_inds[-1]]
        for index in reversed(list(range(1, self.num_obs))):
            max_inds[index - 1] = self.trans_id[max_inds[index], index]
            hmm_fit[index - 1] = self.states[max_inds[index - 1]]
        return hmm_fit

    def _fit(self, X, Y=None):
        """Do nothing, currently empty."""
        return self

    def _predict(self, X):
        """Determine the most likely seq of hidden states by Viterbi algorithm."""
        self.num_states = len(self.emission_funcs)
        self.states = [i for i in range(self.num_states)]
        self.num_obs = len(X)
        emi_probs = HMM._make_emission_probs(self.emission_funcs, X)
        init_probs = self.initial_probs
        # if no initial_probs were supplied assign all states equal prob:
        if self.initial_probs is None:
            init_probs = 1.0 / (self.num_states) * np.ones(self.num_states)
        trans_prob, trans_id = HMM._calculate_trans_mats(
            init_probs,
            emi_probs,
            self.transition_prob_mat,
            self.num_obs,
            self.num_states,
        )
        self.trans_prob = trans_prob
        self.trans_id = trans_id
        return self._hmm_viterbi_label()

    @classmethod
    def get_test_params(cls, parameter_set="default"):
        """Return testing parameter settings for the estimator.

        Parameters
        ----------
        parameter_set : str, default="default"
            Name of the set of test parameters to return, for use in tests. If no
            special parameters are defined for a value, will return `"default"` set.

        Returns
        -------
        params : dict or list of dict
        """
        from numpy import asarray
        from scipy.stats import norm

        # define the emission probs for our HMM model:
        centers = [3.5, -5]
        sd = [0.25 for i in centers]
        emi_funcs = [
            (norm.pdf, {"loc": mean, "scale": sd[ind]})
            for ind, mean in enumerate(centers)
        ]
        # make the transition_mat:
        trans_mat = asarray([[0.25, 0.75], [0.666, 0.333]])
        params_1 = {"emission_funcs": emi_funcs, "transition_prob_mat": trans_mat}
        # also try with passing initial_probs:
        params_2 = {
            "emission_funcs": emi_funcs,
            "transition_prob_mat": trans_mat,
            "initial_probs": asarray([0.2, 0.8]),
        }
        return [params_1, params_2]
