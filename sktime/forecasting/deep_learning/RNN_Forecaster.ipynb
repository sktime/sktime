{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Multi Layer Perceptron Network (MLP) for forecasting.\"\"\"\n",
    "\n",
    "from sktime.forecasting.deep_learning.base import BaseDeepForecastor\n",
    "from sktime.networks.mlp import MLPNetwork\n",
    "from sktime.utils.validation._dependencies import _check_dl_dependencies\n",
    "\n",
    "_check_dl_dependencies(severity=\"warning\")\n",
    "\n",
    "\n",
    "class MLPForecaster(BaseDeepForecastor):\n",
    "    \"\"\"Multi Layer Perceptron Network (MLP), derived from [1].\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_epochs       : int, default = 2000\n",
    "        the number of epochs to train the model\n",
    "    batch_size      : int, default = 4\n",
    "        the number of samples per gradient update.\n",
    "    steps           : int, default = 3\n",
    "        the lookback window for forecasting.\n",
    "    random_state    : int or None, default=None\n",
    "        Seed for random number generation.\n",
    "    verbose         : boolean, default = False\n",
    "        whether to output extra information\n",
    "    loss            : string, default=\"mean_squared_error\"\n",
    "        fit parameter for the keras model\n",
    "    optimizer       : keras.optimizer, default=keras.optimizers.Adam(),\n",
    "    metrics         : list of strings, default=[\"accuracy\"],\n",
    "    activation      : string or a tf callable, default=\"sigmoid\"\n",
    "        Activation function used in the output linear layer.\n",
    "        List of available activation functions:\n",
    "        https://keras.io/api/layers/activations/\n",
    "    use_bias        : boolean, default = True\n",
    "        whether the layer uses a bias vector.\n",
    "    optimizer       : keras.optimizers object, default = Adam(lr=0.01)\n",
    "        specify the optimizer and the learning rate to be used.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    .. .. [1]  Network originally defined in:\n",
    "    @inproceedings{wang2017time, title={Time series classification from\n",
    "    scratch with deep neural networks: A strong baseline}, author={Wang,\n",
    "    Zhiguang and Yan, Weizhong and Oates, Tim}, booktitle={2017\n",
    "    International joint conference on neural networks (IJCNN)}, pages={\n",
    "    1578--1585}, year={2017}, organization={IEEE} }\n",
    "\n",
    "    Derived from the implementation from source code\n",
    "    https://github.com/hfawaz/dl-4-tsc/blob/master/classifiers/mlp.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_epochs=200,\n",
    "        batch_size=4,\n",
    "        steps=3,\n",
    "        callbacks=None,\n",
    "        verbose=False,\n",
    "        loss=\"mse\",\n",
    "        metrics=None,\n",
    "        random_state=None,\n",
    "        activation=\"relu\",\n",
    "        use_bias=True,\n",
    "        optimizer=None,\n",
    "    ):\n",
    "        _check_dl_dependencies(severity=\"error\")\n",
    "        super(MLPForecaster, self).__init__()\n",
    "        self.callbacks = callbacks\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = steps\n",
    "        self.verbose = verbose\n",
    "        self.loss = loss\n",
    "        self.metrics = metrics\n",
    "        self.random_state = random_state\n",
    "        self.activation = activation\n",
    "        self.use_bias = use_bias\n",
    "        self.optimizer = optimizer\n",
    "        self.history = None\n",
    "        self._network = MLPNetwork()\n",
    "\n",
    "    def build_model(self, input_shape, **kwargs):\n",
    "        \"\"\"Construct a compiled, un-trained, keras model that is ready for training.\n",
    "\n",
    "        In sktime, time series are stored in numpy arrays of shape (d,m), where d\n",
    "        is the number of dimensions, m is the series length. Keras/tensorflow assume\n",
    "        data is in shape (m,d). This method also assumes (m,d). Transpose should\n",
    "        happen in fit.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_shape : tuple\n",
    "            The shape of the data fed into the input layer, should be (m,d)\n",
    "        n_classes: int\n",
    "            The number of classes, which becomes the size of the output layer\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output : a compiled Keras Model\n",
    "        \"\"\"\n",
    "        from tensorflow import keras\n",
    "\n",
    "        self.metrics = [\"accuracy\"] if self.metrics is None else self.metrics\n",
    "        input_layer, output_layer = self._network.build_network(\n",
    "            input_shape,\n",
    "        )\n",
    "        output_layer = keras.layers.Dense(units=1, activation=self.activation)(\n",
    "            output_layer\n",
    "        )\n",
    "\n",
    "        self.optimizer_ = (\n",
    "            keras.optimizers.Adam(learning_rate=0.0001)\n",
    "            if self.optimizer is None\n",
    "            else self.optimizer\n",
    "        )\n",
    "\n",
    "        model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "        model.compile(loss=self.loss, optimizer=self.optimizer_, metrics=self.metrics)\n",
    "        return model\n",
    "\n",
    "    def _fit(self, y, fh=None, X=None):\n",
    "        \"\"\"Fit the forecaster on the training set (y) with exog data (X).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y: np.array of shape = (n_instances (n))\n",
    "            The main data which needs to be predicted.\n",
    "        fh: list of int\n",
    "            Forecasting Horizon for the forecaster.\n",
    "        X: np.ndarray of shape = (n_instances (n), exog_dimensions (d))\n",
    "            Exogeneous data for data prediction.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self: object\n",
    "        \"\"\"\n",
    "        import numpy as np\n",
    "\n",
    "        source, target = self.splitSeq(self.steps, y)\n",
    "        if X is not None:\n",
    "            src_x, _ = self.splitSeq(self.steps, X)\n",
    "            # currently takes care of cases where exog data is\n",
    "            # greater than 1 in length\n",
    "            source = [\n",
    "                [_sx + [_sy] for _sx, _sy in zip(sx, sy)]\n",
    "                for sx, sy in zip(src_x, source)\n",
    "            ]\n",
    "\n",
    "        source, target = np.array(source), np.array(target)\n",
    "        if X is None:\n",
    "            source = source.reshape((*source.shape, 1))\n",
    "        source = source.transpose(0, 2, 1)\n",
    "        self.input_shape = source.shape[1:]\n",
    "        self.source, self.target = source, target\n",
    "\n",
    "        self.model_ = self.build_model(self.input_shape)\n",
    "        if self.verbose:\n",
    "            self.model_.summary()\n",
    "\n",
    "        self.history = self.model_.fit(\n",
    "            source,\n",
    "            target,\n",
    "            batch_size=self.batch_size,\n",
    "            epochs=self.n_epochs,\n",
    "            verbose=self.verbose,\n",
    "            callbacks=self.callbacks,\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def _predict(self, fh, X=None):\n",
    "        \"\"\"Get predictions for steps mentioned in fh based on given y and X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        fh: list of int\n",
    "            Forecasting Horizon for the forecaster.\n",
    "        X: np.ndarray of shape = (n_instances (n), exog_dimensions (d))\n",
    "            Exogeneous data for data prediction.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        fvalues: list with predictions of relevant fh.\n",
    "        \"\"\"\n",
    "        import numpy as np\n",
    "\n",
    "        currentPred = 1\n",
    "        lastPred = max(fh)\n",
    "        fvalues = []\n",
    "        fh = set(fh)\n",
    "        source = self.source[-1]\n",
    "        source = source[np.newaxis, :, :]\n",
    "        while currentPred <= lastPred:\n",
    "            yhat = self.model_.predict(source)\n",
    "            source = np.delete(source, axis=2, obj=0)\n",
    "            if X is not None:\n",
    "                source = np.insert(\n",
    "                    source,\n",
    "                    obj=source.shape[-1],\n",
    "                    values=[*exog_p[currentPred - 1], yhat],\n",
    "                    axis=-1,\n",
    "                )\n",
    "            else:\n",
    "                source = np.insert(source, obj=source.shape[-1], values=yhat, axis=-1)\n",
    "            if currentPred in fh:\n",
    "                fvalues.append(yhat)\n",
    "\n",
    "            currentPred += 1\n",
    "        return fvalues\n",
    "\n",
    "    def splitSeq(self, steps, seq):\n",
    "        \"\"\"Get window sized instances of sequence.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        steps: int\n",
    "            Window Size of the forecaster.\n",
    "        seq: np.ndarray of shape = (n_instances (n), n_dimensions (d))\n",
    "            Data to split in window-sized instances.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        source: list containing the data on which model is trained.\n",
    "        target: list of future predictions of data.\n",
    "        \"\"\"\n",
    "        source, target = [], []\n",
    "        for i in range(len(seq)):\n",
    "            end_idx = i + steps\n",
    "            if end_idx > len(seq) - 1:\n",
    "                break\n",
    "            seq_src, seq_tgt = seq[i:end_idx], seq[end_idx]\n",
    "            source.append(seq_src)\n",
    "            target.append(seq_tgt)\n",
    "        return source, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "exog = []\n",
    "for i in range(len(raw_seq)):\n",
    "    exog.append([i, i + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcn = MLPForecaster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPForecaster(metrics=[&#x27;accuracy&#x27;])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPForecaster</label><div class=\"sk-toggleable__content\"><pre>MLPForecaster(metrics=[&#x27;accuracy&#x27;])</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPForecaster(metrics=['accuracy'])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcn._fit(y=raw_seq, X=exog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [3883.087890625,\n",
       "  3189.367919921875,\n",
       "  3026.829833984375,\n",
       "  2806.221435546875,\n",
       "  2514.705078125,\n",
       "  2194.936767578125,\n",
       "  2227.121826171875,\n",
       "  1252.4317626953125,\n",
       "  1096.2259521484375,\n",
       "  981.8132934570312,\n",
       "  719.13623046875,\n",
       "  636.0213012695312,\n",
       "  319.41802978515625,\n",
       "  221.5382843017578,\n",
       "  333.3432922363281,\n",
       "  493.6846008300781,\n",
       "  119.61406707763672,\n",
       "  196.69720458984375,\n",
       "  64.91850280761719,\n",
       "  258.434326171875,\n",
       "  304.7683410644531,\n",
       "  218.7667694091797,\n",
       "  65.25494384765625,\n",
       "  70.20673370361328,\n",
       "  88.60517120361328,\n",
       "  165.02186584472656,\n",
       "  127.563720703125,\n",
       "  268.02886962890625,\n",
       "  148.12831115722656,\n",
       "  265.0610046386719,\n",
       "  168.50340270996094,\n",
       "  109.50827026367188,\n",
       "  111.7464828491211,\n",
       "  220.85565185546875,\n",
       "  143.10516357421875,\n",
       "  136.03765869140625,\n",
       "  38.81959533691406,\n",
       "  124.9878158569336,\n",
       "  176.6869659423828,\n",
       "  81.2987289428711,\n",
       "  181.4322509765625,\n",
       "  96.5024642944336,\n",
       "  198.1737518310547,\n",
       "  162.8346710205078,\n",
       "  183.6537628173828,\n",
       "  112.6746826171875,\n",
       "  112.7409896850586,\n",
       "  58.81935119628906,\n",
       "  241.0333251953125,\n",
       "  57.4794807434082,\n",
       "  138.20651245117188,\n",
       "  156.3288116455078,\n",
       "  151.43092346191406,\n",
       "  185.77500915527344,\n",
       "  357.1752014160156,\n",
       "  51.67106246948242,\n",
       "  74.82335662841797,\n",
       "  73.46170043945312,\n",
       "  73.1836166381836,\n",
       "  136.9027557373047,\n",
       "  68.7435302734375,\n",
       "  248.13807678222656,\n",
       "  56.87002944946289,\n",
       "  125.19681549072266,\n",
       "  33.94883728027344,\n",
       "  54.29422378540039,\n",
       "  20.979595184326172,\n",
       "  26.838014602661133,\n",
       "  61.03889465332031,\n",
       "  12.210614204406738,\n",
       "  210.33428955078125,\n",
       "  71.73117065429688,\n",
       "  34.9694938659668,\n",
       "  66.57052612304688,\n",
       "  228.3084259033203,\n",
       "  23.92722511291504,\n",
       "  90.6864013671875,\n",
       "  236.68211364746094,\n",
       "  236.08396911621094,\n",
       "  68.5745849609375,\n",
       "  115.84757232666016,\n",
       "  155.2137908935547,\n",
       "  65.593017578125,\n",
       "  89.63522338867188,\n",
       "  117.16583251953125,\n",
       "  122.7122802734375,\n",
       "  60.34663772583008,\n",
       "  56.19266891479492,\n",
       "  104.83101654052734,\n",
       "  84.62793731689453,\n",
       "  48.9061279296875,\n",
       "  203.3942108154297,\n",
       "  79.81712341308594,\n",
       "  68.58235168457031,\n",
       "  99.12793731689453,\n",
       "  96.55553436279297,\n",
       "  56.24471664428711,\n",
       "  238.774169921875,\n",
       "  77.87677001953125,\n",
       "  39.7463264465332,\n",
       "  171.09515380859375,\n",
       "  31.639846801757812,\n",
       "  26.345413208007812,\n",
       "  172.6513214111328,\n",
       "  135.0904998779297,\n",
       "  153.03915405273438,\n",
       "  219.32276916503906,\n",
       "  142.98556518554688,\n",
       "  43.62263107299805,\n",
       "  49.49861145019531,\n",
       "  165.1436767578125,\n",
       "  42.376094818115234,\n",
       "  39.87083435058594,\n",
       "  95.1845474243164,\n",
       "  28.435483932495117,\n",
       "  159.87596130371094,\n",
       "  396.39453125,\n",
       "  22.78143310546875,\n",
       "  92.74081420898438,\n",
       "  126.02581787109375,\n",
       "  16.743349075317383,\n",
       "  68.64513397216797,\n",
       "  194.4035186767578,\n",
       "  60.09354782104492,\n",
       "  105.12346649169922,\n",
       "  127.093505859375,\n",
       "  76.36121368408203,\n",
       "  86.97669219970703,\n",
       "  109.44723510742188,\n",
       "  40.1193962097168,\n",
       "  96.82219696044922,\n",
       "  94.50658416748047,\n",
       "  129.14036560058594,\n",
       "  14.00284194946289,\n",
       "  60.52967834472656,\n",
       "  116.5962142944336,\n",
       "  95.65664672851562,\n",
       "  73.9070053100586,\n",
       "  8.000004768371582,\n",
       "  48.11962890625,\n",
       "  70.6730728149414,\n",
       "  77.33448028564453,\n",
       "  40.55263900756836,\n",
       "  239.73577880859375,\n",
       "  62.54612350463867,\n",
       "  39.628170013427734,\n",
       "  66.88702392578125,\n",
       "  57.5518913269043,\n",
       "  105.33138275146484,\n",
       "  120.1768798828125,\n",
       "  103.1812973022461,\n",
       "  13.971880912780762,\n",
       "  63.46527099609375,\n",
       "  40.59492111206055,\n",
       "  70.89789581298828,\n",
       "  94.39290618896484,\n",
       "  48.436279296875,\n",
       "  19.205951690673828,\n",
       "  34.58427047729492,\n",
       "  109.62604522705078,\n",
       "  54.8564567565918,\n",
       "  216.5574951171875,\n",
       "  57.68850326538086,\n",
       "  50.179561614990234,\n",
       "  93.5970458984375,\n",
       "  258.4385070800781,\n",
       "  95.1456069946289,\n",
       "  45.6910400390625,\n",
       "  75.40232849121094,\n",
       "  34.714874267578125,\n",
       "  62.61220169067383,\n",
       "  77.55459594726562,\n",
       "  254.8150634765625,\n",
       "  51.6385383605957,\n",
       "  24.4710750579834,\n",
       "  92.51949310302734,\n",
       "  32.9211540222168,\n",
       "  61.81278610229492,\n",
       "  8.74191951751709,\n",
       "  135.6304168701172,\n",
       "  43.29838943481445,\n",
       "  155.10150146484375,\n",
       "  63.76533126831055,\n",
       "  54.81691360473633,\n",
       "  36.71120071411133,\n",
       "  173.2205047607422,\n",
       "  49.64067077636719,\n",
       "  94.24422454833984,\n",
       "  111.28795623779297,\n",
       "  166.03028869628906,\n",
       "  31.56719970703125,\n",
       "  13.496231079101562,\n",
       "  31.776443481445312,\n",
       "  108.48812103271484,\n",
       "  67.20405578613281,\n",
       "  23.27961540222168,\n",
       "  23.434675216674805,\n",
       "  37.625423431396484,\n",
       "  34.50282287597656,\n",
       "  60.85928726196289],\n",
       " 'accuracy': [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0]}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcn.history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[9, 10], [10, 11], [11, 12], [12, 13]]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exog_p = []\n",
    "for i in range(len(raw_seq), len(raw_seq) + 4):\n",
    "    exog_p.append([i, i + 1])\n",
    "exog_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 104ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| source.shape: (1, 3, 3)\n",
      "ic| source.shape: (1, 3, 2)\n",
      "ic| exog_p[currentPred-1]: [9, 10]\n",
      "ic| source.shape: (1, 3, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| source.shape: (1, 3, 3)\n",
      "ic| source.shape: (1, 3, 2)\n",
      "ic| exog_p[currentPred-1]: [10, 11]\n",
      "ic| source.shape: (1, 3, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 27ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| source.shape: (1, 3, 3)\n",
      "ic| source.shape: (1, 3, 2)\n",
      "ic| exog_p[currentPred-1]: [11, 12]\n",
      "ic| source.shape: (1, 3, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| source.shape: (1, 3, 3)\n",
      "ic| source.shape: (1, 3, 2)\n",
      "ic| exog_p[currentPred-1]: [12, 13]\n",
      "ic| source.shape: (1, 3, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[79.4205]], dtype=float32),\n",
       " array([[86.35225]], dtype=float32),\n",
       " array([[92.918106]], dtype=float32),\n",
       " array([[97.81372]], dtype=float32)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcn._predict(fh=[1, 2, 3, 4], X=exog_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "interpreter": {
   "hash": "e0810de9a50ad13c45b0d4dc7d175c8f87a08de49fe82453ca733c71cdd5f2ad"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 ('sktime-dev': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
